{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a051ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = pathlib.Path(\"por-eng/por.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb52fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52397ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split(\"\\t\") for line in lines]\n",
    "\n",
    "    context = np.array([context for target, context, _ in pairs])\n",
    "    target = np.array([target for target, context, _ in pairs])\n",
    "\n",
    "    return context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601095b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "portuguese_sentences, english_sentences = load_data(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82583472",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = (portuguese_sentences, english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d44de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(english_sentences)\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54325171",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = np.random.uniform(size=(len(portuguese_sentences),)) < 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a25356",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (english_sentences[is_train], portuguese_sentences[is_train])\n",
    "    )\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "val_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (english_sentences[~is_train], portuguese_sentences[~is_train])\n",
    "    )\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,¿]\", \"\")\n",
    "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25081109",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vectorizer.adapt(train_raw.map(lambda context, target: context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "portuguese_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "portuguese_vectorizer.adapt(train_raw.map(lambda context, target: target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    context = english_vectorizer(context).to_tensor()\n",
    "    target = portuguese_vectorizer(target)\n",
    "    targ_in = target[:, :-1].to_tensor()\n",
    "    targ_out = target[:, 1:].to_tensor()\n",
    "    return (context, targ_in), targ_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9060f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_data = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36060aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_raw\n",
    "del val_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2007e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    \n",
    "    # Check which elements of y_true are padding\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    \n",
    "    loss *= mask\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41709896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    match*= mask\n",
    "\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a947a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(tokens, id_to_word):\n",
    "    words = id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis=-1, separator=\" \")\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
