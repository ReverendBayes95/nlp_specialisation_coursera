{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ea73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from utils import PositionalEmbedding\n",
    "from dlai_grader.grading import test_case, object_to_grade\n",
    "from types import ModuleType, FunctionType\n",
    "\n",
    "# Compare the two inputs\n",
    "\n",
    "def comparator(learner, instructor, modelId):\n",
    "    cases = []\n",
    "    t = test_case()\n",
    "    if len(learner) != len(instructor):\n",
    "        t.failed = True\n",
    "        t.msg = f\"{modelId}: The number of layers in the proposed model does not agree with the expected model\"\n",
    "        t.want = len(instructor)\n",
    "        t.got = len(learner)\n",
    "    cases.append(t)\n",
    "    index_layer = 1\n",
    "\n",
    "    for a, b in zip(learner, instructor):\n",
    "        t = test_case()\n",
    "        if tuple(a) != tuple(b):\n",
    "            t.failed = True\n",
    "            t.msg = f\"{modelId}: Test failed in layer {index_layer}\"\n",
    "            t.want = b\n",
    "            t.got = a\n",
    "        cases.append(t)\n",
    "        index_layer = index_layer + 1\n",
    "    return cases\n",
    "\n",
    "\n",
    "def summary(model):\n",
    "    result = []\n",
    "    for layer in model.layers:\n",
    "        descriptors = [layer.__class__.__name__,\n",
    "                       layer.output_shape, layer.count_params()]\n",
    "        if (type(layer) == Dense):\n",
    "            descriptors.append(layer.activation.__name__)\n",
    "        if (type(layer) == Dropout):\n",
    "            descriptors.append(f\"rate={layer.rate}\")\n",
    "        if (type(layer) == GRU):\n",
    "            descriptors.append(f\"return_sequences={layer.return_sequences}\")\n",
    "            descriptors.append(f\"return_state={layer.return_state}\")\n",
    "        if (type(layer) == PositionalEmbedding):\n",
    "            descriptors.append(f\"vocab_size={layer.vocab_size}\")\n",
    "            descriptors.append(f\"d_model={layer.d_model}\")\n",
    "            descriptors.append(f\"max_length={layer.max_length}\")\n",
    "        if hasattr(layer, 'd_model'):\n",
    "            descriptors.append(f\"d_model={layer.d_model}\")\n",
    "        if hasattr(layer, 'd_ff'):\n",
    "            descriptors.append(f\"d_ff={layer.d_ff}\")\n",
    "        if hasattr(layer, 'n_heads'):\n",
    "            descriptors.append(f\"n_heads={layer.n_heads}\")\n",
    "        if hasattr(layer, 'dropout'):\n",
    "            descriptors.append(f\"dropout={layer.dropout}\")\n",
    "        if hasattr(layer, 'ff_activation'):\n",
    "            descriptors.append(f\"ff_activation={layer.ff_activation}\")\n",
    "        \n",
    "        result.append(descriptors)\n",
    "    return result\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
