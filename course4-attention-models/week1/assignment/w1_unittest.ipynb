{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ddb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import combinations\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from dlai_grader.grading import test_case, print_feedback\n",
    "from utils import train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eec3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12000\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d87c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoder(encoder_to_test):\n",
    "    def g():\n",
    "        vocab_sizes = [5, 20, 1000, 15000]\n",
    "        units = [32, 64, 256, 512]\n",
    "\n",
    "        cases = []\n",
    "\n",
    "        encoder = encoder_to_test(vocab_sizes[0], units[0])\n",
    "\n",
    "        t = test_case()\n",
    "        if encoder.embedding.mask_zero != True:\n",
    "            t.failed = True\n",
    "            t.msg = \"Embedding layer has incorrect value for 'mask_zero' attribute\"\n",
    "            t.want = True\n",
    "            t.got = encoder.embedding.mask_zero\n",
    "        cases.append(t)\n",
    "\n",
    "        for vs, u in zip(vocab_sizes, units):\n",
    "            encoder = encoder_to_test(vs, u)\n",
    "\n",
    "            t = test_case()\n",
    "            if encoder.embedding.input_dim != vs:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect input dim of embedding layer\"\n",
    "                t.want = vs\n",
    "                t.got = encoder.embedding.input_dim\n",
    "            cases.append(t)\n",
    "\n",
    "            t = test_case()\n",
    "            if encoder.embedding.output_dim != u:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect output dim of embedding layer\"\n",
    "                t.want = u\n",
    "                t.got = encoder.embedding.output_dim\n",
    "            cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if not isinstance(encoder.rnn.layer, tf.keras.layers.LSTM):\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect type of layer inside Bidirectional\"\n",
    "            t.want = tf.keras.layers.LSTM\n",
    "            t.got = type(encoder.rnn.layer)\n",
    "            return [t]\n",
    "\n",
    "        for u in units:\n",
    "            encoder = encoder_to_test(vocab_sizes[1], u)\n",
    "            t = test_case()\n",
    "            if encoder.rnn.layer.units != u:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect number of units in LSTM layer\"\n",
    "                t.want = u\n",
    "                t.got = encoder.rnn.layer.units\n",
    "            cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if encoder.rnn.layer.return_sequences != True:\n",
    "            t.failed = True\n",
    "            t.msg = \"LSTM layer has incorrect value for 'return_sequences' attribute\"\n",
    "            t.want = True\n",
    "            t.got = encoder.rnn.layer.return_sequences\n",
    "        cases.append(t)\n",
    "\n",
    "        vocab_size = 16\n",
    "        n_units = 8\n",
    "        encoder = encoder_to_test(vocab_size, n_units)\n",
    "        to_translate = np.array([[1, 2, 3, 4, 5, 6, 14, 0, 0, 0],\n",
    "                               [2, 1, 1, 1, 1, 1, 8, 0, 0, 0],\n",
    "                               [5, 4, 2, 3, 3, 15, 11, 0, 0, 0]])\n",
    "        #for (to_translate, _), _ in train_data.take(3):\n",
    "            \n",
    "        first_dim_in, second_dim_in = to_translate.shape\n",
    "        encoder_output = encoder(to_translate)\n",
    "        t = test_case()\n",
    "        if len(encoder_output.shape) != 3:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect shape of encoder output\"\n",
    "            t.want = \"a shape with 3 dimensions\"\n",
    "            t.got = encoder_output.shape\n",
    "            return [t]\n",
    "\n",
    "        first_dim_out, second_dim_out, third_dim_out = encoder_output.shape\n",
    "\n",
    "        t = test_case()\n",
    "        if first_dim_in != first_dim_out:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect first dimension of encoder output\"\n",
    "            t.want = first_dim_in\n",
    "            t.got = first_dim_out\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if second_dim_in != second_dim_out:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect second dimension of encoder output\"\n",
    "            t.want = second_dim_in\n",
    "            t.got = second_dim_out\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if third_dim_out != n_units:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect third dimension of encoder output\"\n",
    "            t.want = units\n",
    "            t.got = third_dim_out\n",
    "        cases.append(t)\n",
    "\n",
    "        return cases\n",
    "\n",
    "    cases = g()\n",
    "    print_feedback(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cross_attention(cross_attention_to_test):\n",
    "    def g():\n",
    "        units = [32, 64, 256, 512]\n",
    "\n",
    "        cases = []\n",
    "\n",
    "        n_units = 512\n",
    "        cross_attention = cross_attention_to_test(n_units)\n",
    "\n",
    "        t = test_case()\n",
    "        if not isinstance(cross_attention.mha, tf.keras.layers.MultiHeadAttention):\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect type of layer for Multi Head Attention\"\n",
    "            t.want = tf.keras.layers.MultiHeadAttention\n",
    "            t.got = type(cross_attention.mha)\n",
    "            return [t]\n",
    "\n",
    "        #         for u in units:\n",
    "        #             cross_attention = cross_attention_to_test(u)\n",
    "\n",
    "        #             t = test_case()\n",
    "        #             if cross_attention.mha.key_dim != u:\n",
    "        #                 t.failed = True\n",
    "        #                 t.msg = \"Incorrect key dim of Multi Head Attention layer\"\n",
    "        #                 t.want = u\n",
    "        #                 t.got = cross_attention.mha.key_dim\n",
    "        #             cases.append(t)\n",
    "\n",
    "        cross_attention = cross_attention_to_test(n_units)\n",
    "        embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=UNITS, mask_zero=True)\n",
    "\n",
    "        for (to_translate, sr_translation), _ in train_data.take(3):\n",
    "            sr_translation_embed = embed(sr_translation)\n",
    "            first_dim_in, second_dim_in, third_dim_in = sr_translation_embed.shape\n",
    "            dummy_encoder_output = np.random.rand(64, 14, 512)\n",
    "            cross_attention_output = cross_attention(\n",
    "                dummy_encoder_output, sr_translation_embed\n",
    "            )\n",
    "            #             print(cross_attention_output.shape)\n",
    "\n",
    "            t = test_case()\n",
    "            if len(cross_attention_output.shape) != 3:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect shape of cross_attention output\"\n",
    "                t.want = \"a shape with 3 dimensions\"\n",
    "                t.got = cross_attention_output.shape\n",
    "                return [t]\n",
    "\n",
    "            first_dim_out, second_dim_out, third_dim_out = cross_attention_output.shape\n",
    "\n",
    "            t = test_case()\n",
    "            if first_dim_in != first_dim_out:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect first dimension of cross_attention output\"\n",
    "                t.want = first_dim_in\n",
    "                t.got = first_dim_out\n",
    "            cases.append(t)\n",
    "\n",
    "            t = test_case()\n",
    "            if second_dim_in != second_dim_out:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect second dimension of cross_attention output\"\n",
    "                t.want = second_dim_in\n",
    "                t.got = second_dim_out\n",
    "            cases.append(t)\n",
    "\n",
    "            t = test_case()\n",
    "            if third_dim_in != third_dim_out:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect third dimension of cross_attention output\"\n",
    "                t.want = third_dim_in\n",
    "                t.got = third_dim_out\n",
    "            cases.append(t)\n",
    "\n",
    "        _, n_heads, key_dim = cross_attention.mha.get_weights()[0].shape\n",
    "\n",
    "        t = test_case()\n",
    "        if n_heads != 1:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect number of attention heads\"\n",
    "            t.want = 1\n",
    "            t.got = n_heads\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if key_dim != n_units:\n",
    "            t.failed = True\n",
    "            t.msg = f\"Incorrect size of query and key for every attention head when passing {n_units} units to the constructor\"\n",
    "            t.want = n_units\n",
    "            t.got = key_dim\n",
    "        cases.append(t)\n",
    "\n",
    "        return cases\n",
    "\n",
    "    cases = g()\n",
    "    print_feedback(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decoder(decoder_to_test, CrossAttention):\n",
    "    def g():\n",
    "        vocab_sizes = [5, 20, 1000, 15000]\n",
    "        units = [32, 64, 256, 512]\n",
    "\n",
    "        cases = []\n",
    "\n",
    "        vocab_size = 10000\n",
    "        n_units = 512\n",
    "        decoder = decoder_to_test(vocab_size, n_units)\n",
    "\n",
    "        t = test_case()\n",
    "        if not isinstance(decoder.embedding, tf.keras.layers.Embedding):\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect type of embedding layer\"\n",
    "            t.want = tf.keras.layers.Embedding\n",
    "            t.got = type(decoder.embedding)\n",
    "            return [t]\n",
    "\n",
    "        t = test_case()\n",
    "        if decoder.embedding.mask_zero != True:\n",
    "            t.failed = True\n",
    "            t.msg = \"Embedding layer has incorrect value for 'mask_zero' attribute\"\n",
    "            t.want = True\n",
    "            t.got = decoder.embedding.mask_zero\n",
    "        cases.append(t)\n",
    "\n",
    "        for vs, u in zip(vocab_sizes, units):\n",
    "            decoder = decoder_to_test(vs, u)\n",
    "\n",
    "            t = test_case()\n",
    "            if decoder.embedding.input_dim != vs:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect input dim of embedding layer\"\n",
    "                t.want = vs\n",
    "                t.got = decoder.embedding.input_dim\n",
    "            cases.append(t)\n",
    "\n",
    "            t = test_case()\n",
    "            if decoder.embedding.output_dim != u:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect output dim of embedding layer\"\n",
    "                t.want = u\n",
    "                t.got = decoder.embedding.output_dim\n",
    "            cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if not isinstance(decoder.pre_attention_rnn, tf.keras.layers.LSTM):\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect type of pre_attention_rnn layer\"\n",
    "            t.want = tf.keras.layers.LSTM\n",
    "            t.got = type(decoder.pre_attention_rnn)\n",
    "            return [t]\n",
    "\n",
    "        for u in units:\n",
    "            decoder = decoder_to_test(vocab_size, u)\n",
    "            t = test_case()\n",
    "            if decoder.pre_attention_rnn.units != u:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect number of units in pre_attention_rnn layer\"\n",
    "                t.want = u\n",
    "                t.got = decoder.pre_attention_rnn.units\n",
    "            cases.append(t)\n",
    "\n",
    "            #             t = test_case()\n",
    "            #             if decoder.attention.units != u:\n",
    "            #                 t.failed = True\n",
    "            #                 t.msg = \"Incorrect number of units in attention layer\"\n",
    "            #                 t.want = u\n",
    "            #                 t.got = decoder.attention.units\n",
    "            #             cases.append(t)\n",
    "\n",
    "            t = test_case()\n",
    "            if decoder.post_attention_rnn.units != u:\n",
    "                t.failed = True\n",
    "                t.msg = \"Incorrect number of units in post_attention_rnn layer\"\n",
    "                t.want = u\n",
    "                t.got = decoder.post_attention_rnn.units\n",
    "            cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if decoder.pre_attention_rnn.return_sequences != True:\n",
    "            t.failed = True\n",
    "            t.msg = \"pre_attention_rnn layer has incorrect value for 'return_sequences' attribute\"\n",
    "            t.want = True\n",
    "            t.got = decoder.pre_attention_rnn.return_sequences\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if decoder.pre_attention_rnn.return_state != True:\n",
    "            t.failed = True\n",
    "            t.msg = \"pre_attention_rnn layer has incorrect value for 'return_state' attribute\"\n",
    "            t.want = True\n",
    "            t.got = decoder.pre_attention_rnn.return_state\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if not isinstance(decoder.attention, CrossAttention):\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect type of attention layer\"\n",
    "            t.want = CrossAttention\n",
    "            t.got = type(decoder.attention)\n",
    "            return [t]\n",
    "\n",
    "        t = test_case()\n",
    "        if decoder.post_attention_rnn.return_sequences != True:\n",
    "            t.failed = True\n",
    "            t.msg = \"post_attention_rnn layer has incorrect value for 'return_sequences' attribute\"\n",
    "            t.want = True\n",
    "            t.got = decoder.post_attention_rnn.return_sequences\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if not isinstance(decoder.post_attention_rnn, tf.keras.layers.LSTM):\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect type of pre_attention_rnn layer\"\n",
    "            t.want = tf.keras.layers.LSTM\n",
    "            t.got = type(decoder.post_attention_rnn)\n",
    "            return [t]\n",
    "\n",
    "        t = test_case()\n",
    "        if not isinstance(decoder.output_layer, tf.keras.layers.Dense):\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect type of output_layer layer\"\n",
    "            t.want = tf.keras.layers.Dense\n",
    "            t.got = type(decoder.output_layer)\n",
    "            return [t]\n",
    "\n",
    "        t = test_case()\n",
    "        if (\n",
    "            \"log\" not in decoder.output_layer.activation.__name__\n",
    "            or \"softmax\" not in decoder.output_layer.activation.__name__\n",
    "        ):\n",
    "            t.failed = True\n",
    "            t.msg = \"output_layer layer has incorrect activation function\"\n",
    "            t.want = \"a log softmax activation function such as 'log_softmax_v2'\"\n",
    "            t.got = decoder.output_layer.activation.__name__\n",
    "        cases.append(t)\n",
    "\n",
    "        vocab_size = 6\n",
    "        n_units = 4\n",
    "        decoder = decoder_to_test(vocab_size, n_units)\n",
    "        sr_translation = np.array([[3, 4, 5, 3, 3, 3, 5, 1, 1, 1, 1, 1], \n",
    "                                    [1, 2, 3, 4, 5, 1, 1, 0, 0, 0, 0, 0]])\n",
    "        encoder_output = np.random.rand(2, 10, n_units)\n",
    "        decoder_output = decoder(encoder_output, sr_translation)\n",
    "\n",
    "        first_dim_in, second_dim_in = sr_translation.shape\n",
    "\n",
    "        t = test_case()\n",
    "        if len(decoder_output.shape) != 3:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect shape of decoder output\"\n",
    "            t.want = \"a shape with 3 dimensions\"\n",
    "            t.got = decoder_output.shape\n",
    "            return [t]\n",
    "\n",
    "        first_dim_out, second_dim_out, third_dim_out = decoder_output.shape\n",
    "\n",
    "        t = test_case()\n",
    "        if first_dim_in != first_dim_out:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect first dimension of decoder output\"\n",
    "            t.want = first_dim_in\n",
    "            t.got = first_dim_out\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if second_dim_in != second_dim_out:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect second dimension of decoder output\"\n",
    "            t.want = second_dim_in\n",
    "            t.got = second_dim_out\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if third_dim_out != vocab_size:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect third dimension of decoder output\"\n",
    "            t.want = vocab_size\n",
    "            t.got = third_dim_out\n",
    "        cases.append(t)\n",
    "        \n",
    "        return cases\n",
    "\n",
    "    cases = g()\n",
    "    print_feedback(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81015fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translator(translator_to_test, Encoder, Decoder):\n",
    "    def g():\n",
    "        vocab_sizes = [5, 20, 1000, 15000]\n",
    "        units = [32, 64, 256, 512]\n",
    "\n",
    "        cases = []\n",
    "\n",
    "        vocab_size = 10000\n",
    "        n_units = 512\n",
    "        translator = translator_to_test(vocab_size, n_units)\n",
    "\n",
    "        t = test_case()\n",
    "        if not isinstance(translator.encoder, Encoder):\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect type of encoder layer\"\n",
    "            t.want = Encoder\n",
    "            t.got = type(translator.encoder)\n",
    "            return [t]\n",
    "\n",
    "        t = test_case()\n",
    "        if not isinstance(translator.decoder, Decoder):\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect type of encoder layer\"\n",
    "            t.want = Decoder\n",
    "            t.got = type(translator.decoder)\n",
    "            return [t]\n",
    "\n",
    "        vocab_size = 16\n",
    "        n_units = 8\n",
    "        translator = translator_to_test(vocab_size, n_units)\n",
    "\n",
    "        to_translate = np. array([[1, 2, 3, 4, 5, 0, 0],\n",
    "                                 [5, 2, 3, 4, 5, 6, 0],\n",
    "                                 [6, 3, 3, 4, 5, 3, 3],\n",
    "                                 [7, 9, 9, 6, 5, 3, 3]])\n",
    "\n",
    "        sr_translation = np. array([[8, 1, 2, 3, 4, 5, 0, 0],\n",
    "                                 [9, 5, 2, 3, 4, 5, 6, 0],\n",
    "                                 [10, 6, 3, 3, 4, 5, 3, 3],\n",
    "                                 [11, 7, 9, 9, 6, 5, 3, 3]])\n",
    "\n",
    "        #for (to_translate, sr_translation), _ in train_data.take(3):\n",
    "        first_dim_in, second_dim_in = sr_translation.shape\n",
    "        translator_output = translator((to_translate, sr_translation))\n",
    "        t = test_case()\n",
    "        if len(translator_output.shape) != 3:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect shape of translator output\"\n",
    "            t.want = \"a shape with 3 dimensions\"\n",
    "            t.got = translator_output.shape\n",
    "            return [t]\n",
    "\n",
    "        first_dim_out, second_dim_out, third_dim_out = translator_output.shape\n",
    "\n",
    "        t = test_case()\n",
    "        if first_dim_in != first_dim_out:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect first dimension of translator output\"\n",
    "            t.want = first_dim_in\n",
    "            t.got = first_dim_out\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if second_dim_in != second_dim_out:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect second dimension of translator output\"\n",
    "            t.want = second_dim_in\n",
    "            t.got = second_dim_out\n",
    "        cases.append(t)\n",
    "\n",
    "        t = test_case()\n",
    "        if third_dim_out != vocab_size:\n",
    "            t.failed = True\n",
    "            t.msg = \"Incorrect third dimension of translator output\"\n",
    "            t.want = vocab_size\n",
    "            t.got = third_dim_out\n",
    "        cases.append(t)\n",
    "\n",
    "        return cases\n",
    "\n",
    "    cases = g()\n",
    "    print_feedback(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22df955",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translate(learner_func, model):\n",
    "    def g():\n",
    "        \n",
    "        cases = []\n",
    "        \n",
    "        txt = \"Hi, my name is Younes\"\n",
    "        try:\n",
    "            translation, logit, tokens = learner_func(model, txt, temperature=0.9)\n",
    "        except Exception as e:\n",
    "            t = test_case()\n",
    "            t.failed = True\n",
    "            t.msg = \"There was an exception when running your function\"\n",
    "            t.want = \"No exceptions\"\n",
    "            t.got = f\"{str(e)}\"\n",
    "            return [t]\n",
    "        \n",
    "        txt = \"Hi, my name is Alejandra\"\n",
    "        translation, logit, tokens = learner_func(model, txt, temperature=0.0)\n",
    "        \n",
    "        t = test_case()\n",
    "        \n",
    "        if not isinstance(translation, str):\n",
    "            t.failed = True\n",
    "            t.msg = \"'translation' has incorrect type\"\n",
    "            t.want = str\n",
    "            t.got = type(translation)\n",
    "        cases.append(t)\n",
    "        \n",
    "        if not isinstance(logit, np.number):\n",
    "            t.failed = True\n",
    "            t.msg = \"'logit' has incorrect type\"\n",
    "            t.want = np.number\n",
    "            t.got = type(logit)\n",
    "        cases.append(t)\n",
    "        \n",
    "        if not isinstance(tokens, tf.Tensor):\n",
    "            t.failed = True\n",
    "            t.msg = \"'tokens' has incorrect type\"\n",
    "            t.want = tf.Tensor\n",
    "            t.got = type(tokens)\n",
    "        cases.append(t)\n",
    "        \n",
    "        translation2, logit2, tokens2 = learner_func(model, txt, temperature=0.0)\n",
    "        \n",
    "        t = test_case()\n",
    "        if translation != translation2:\n",
    "            t.failed = True\n",
    "            t.msg = \"translate didn't return the same translation when using temperature of 0.0\"\n",
    "            t.want = translation\n",
    "            t.got = translation2\n",
    "        cases.append(t)\n",
    "        \n",
    "        t = test_case()\n",
    "        if logit != logit2:\n",
    "            t.failed = True\n",
    "            t.msg = \"translate didn't return the same logit when using temperature of 0.0\"\n",
    "            t.want = logit\n",
    "            t.got = logit2\n",
    "        cases.append(t)\n",
    "        \n",
    "        t = test_case()\n",
    "        if not np.allclose(tokens, tokens2):\n",
    "            t.failed = True\n",
    "            t.msg = \"translate didn't return the same tokens when using temperature of 0.0\"\n",
    "            t.want = tokens\n",
    "            t.got = tokens2\n",
    "        cases.append(t)\n",
    "\n",
    "        # Check that function uses the model.decoder and model.enconder functions\n",
    "        inputs = tf.keras.Input(shape=(37,))\n",
    "        outputs = tf.keras.layers.Dense(5, activation=\"softmax\")(inputs)\n",
    "        model_fake = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "        \n",
    "        model_fake.encoder = model.encoder\n",
    "        model_fake.decoder = None\n",
    "        t = test_case()\n",
    "        try:\n",
    "            ff = learner_func(model_fake, \"Hello world\", temperature=0.0)\n",
    "            t.failed = True\n",
    "            t.msg = \"The translator is not using the internal model.decoder. You are probably using a global variable\"\n",
    "            t.want = \"Fail translation\"\n",
    "            t.got = \"Succeed translation with wrong decoder\"\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "        cases.append(t)\n",
    "        \n",
    "        model_fake.encoder = None\n",
    "        model_fake.decoder = model.decoder\n",
    "        t = test_case()\n",
    "        try:\n",
    "            ff = learner_func(model_fake, \"Hello world\", temperature=0.0)\n",
    "            t.failed = True\n",
    "            t.msg = \"The translator is not using the internal model.encoder. You are probably using a global variable\"\n",
    "            t.want = \"Fail translation\"\n",
    "            t.got = \"Succeed translation with wrong encoder\"\n",
    "        except:\n",
    "            None\n",
    "\n",
    "        cases.append(t)\n",
    "        \n",
    "        return cases\n",
    "    \n",
    "    cases = g()\n",
    "    print_feedback(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523cf8a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rouge1_similarity(learner_func):\n",
    "    \n",
    "    def g():\n",
    "        \n",
    "        tensors = [\n",
    "            [0],\n",
    "            [0, 1],\n",
    "            [0, 1, 2],\n",
    "            [1, 2, 4, 5],\n",
    "            [5, 5, 7, 0, 232]\n",
    "        ]\n",
    "        \n",
    "        expected = [0.6666666666666666, 0.5, 0, 0.33333333333333337, 0.8, 0.3333333333333333, 0.28571428571428575, 0.5714285714285715, 0.25]\n",
    "\n",
    "        cases = []\n",
    "        pairs = list(combinations(tensors, 2))\n",
    "        \n",
    "        for (candidate, reference), solution in zip(pairs, expected):\n",
    "            answer = learner_func(candidate, reference)\n",
    "            t = test_case()\n",
    "            if not math.isclose(answer, solution):\n",
    "                t.failed = True\n",
    "                t.msg = f\"Incorrect similarity for candidate={candidate} and reference={reference}\"\n",
    "                t.want = solution\n",
    "                t.got = answer\n",
    "            cases.append(t)\n",
    "\n",
    "        return cases\n",
    "    \n",
    "    cases = g()\n",
    "    print_feedback(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_average_overlap(learner_func):\n",
    "\n",
    "    def jaccard_similarity(candidate, reference):\n",
    "        \n",
    "        # Convert the lists to sets to get the unique tokens\n",
    "        candidate_set = set(candidate)\n",
    "        reference_set = set(reference)\n",
    "        \n",
    "        # Get the set of tokens common to both candidate and reference\n",
    "        common_tokens = candidate_set.intersection(reference_set)\n",
    "        \n",
    "        # Get the set of all tokens found in either candidate or reference\n",
    "        all_tokens = candidate_set.union(reference_set)\n",
    "        \n",
    "        # Compute the percentage of overlap (divide the number of common tokens by the number of all tokens)\n",
    "        overlap = len(common_tokens) / len(all_tokens)\n",
    "            \n",
    "        return overlap\n",
    "    \n",
    "    def g():\n",
    "        \n",
    "        l1 = [1, 2, 3]\n",
    "        l2 = [1, 2, 4]\n",
    "        l3 = [1, 2, 4, 5]\n",
    "        l4 = [5,6]\n",
    "\n",
    "        elements = [l1, l2, l3, l4]\n",
    "\n",
    "        all_combinations = []\n",
    "\n",
    "        for r in range(2, len(elements) + 1):\n",
    "            # Generate combinations of length r\n",
    "            combinations_r = combinations(elements, r)\n",
    "            \n",
    "            # Append the combinations to the result list\n",
    "            all_combinations.extend(combinations_r)\n",
    "        \n",
    "        expected = [{0: 0.5, 1: 0.5},\n",
    "                     {0: 0.4, 1: 0.4},\n",
    "                     {0: 0.0, 1: 0.0},\n",
    "                     {0: 0.75, 1: 0.75},\n",
    "                     {0: 0.0, 1: 0.0},\n",
    "                     {0: 0.2, 1: 0.2},\n",
    "                     {0: 0.45, 1: 0.625, 2: 0.575},\n",
    "                     {0: 0.25, 1: 0.25, 2: 0.0},\n",
    "                     {0: 0.2, 1: 0.3, 2: 0.1},\n",
    "                     {0: 0.375, 1: 0.475, 2: 0.1},\n",
    "                     {0: 0.3, 1: 0.417, 2: 0.45, 3: 0.067}]\n",
    "\n",
    "        cases = []\n",
    "        \n",
    "        for combination, solution in zip(all_combinations, expected):\n",
    "            answer = learner_func(combination, jaccard_similarity)\n",
    "            t = test_case()\n",
    "            if answer != solution:\n",
    "                t.failed = True\n",
    "                t.msg = f\"Incorrect overlap for lists={combination}\"\n",
    "                t.want = solution\n",
    "                t.got = answer\n",
    "            cases.append(t)\n",
    "\n",
    "        return cases\n",
    "    \n",
    "    cases = g()\n",
    "    print_feedback(cases)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
